<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Scooby Snacks Website</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:100,100i,200,200i,300,300i,400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700,700i" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/business-casual.min.css" rel="stylesheet">

  </head>

  <body>

    <h1 class="site-heading text-center text-white d-none d-lg-block">
      <span class="site-heading-upper text-primary mb-3">Scooby Snacks</span>
      <span class="site-heading-lower">The Premier Robotics Team</span>
    </h1>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark py-lg-4" id="mainNav">
      <div class="container">
        <a class="navbar-brand text-uppercase text-expanded font-weight-bold d-lg-none" href="#">Menu</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav mx-auto">
            <li class="nav-item px-lg-4">
              <a class="nav-link text-uppercase text-expanded" href="index.html">Home
                <span class="sr-only">(current)</span>
              </a>
            </li>
            <li class="nav-item px-lg-4">
              <a class="nav-link text-uppercase text-expanded" href="about.html">About</a>
            </li>
            <li class="nav-item px-lg-4">
              <a class="nav-link text-uppercase text-expanded" href="labs.html">Labs</a>
            </li>
            <li class="nav-item active px-lg-4">
              <a class="nav-link text-uppercase text-expanded" href="milestones.html">Milestones</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <section class="page-section cta">
      <div class="container">
        <div class="row">
          <div class="col-xl-9 mx-auto">
            <div class="cta-inner text-center rounded">
              <h2 class="section-heading mb-5">
                <span class="section-heading-upper">Milestones</span>
              </h2>
              <ul class="list-unstyled list-hours mb-5 text-left mx-auto">
                <li class="list-unstyled-item list-hours-item d-flex">
                  Milestone One
                  <span class="ml-auto">Line Following and Figure Eight</span>
                </li>
                <li class="list-unstyled-item list-hours-item d-flex">
                  Milestone Two
                  <span class="ml-auto">Wall Detection and Signal Detection</span>
                </li>
                <li class="list-unstyled-item list-hours-item d-flex">
                  Milestone Three
                  <span class="ml-auto">Search Algorithm</span>
                </li>
                <li class="list-unstyled-item list-hours-item d-flex">
                  Milestone Four
                  <span class="ml-auto">Treasure Detection and Evaluation</span>
                </li>
              </ul>
             <div align = "middle">
               <p><b><big> MILESTONE 1:</big></b><p>
             <p> <b> <big>Objectives:</big></b> </p>
                <p>One: The robot is able to follow a line and employ self correction to maintain its path. 
                </p>
                <p>Two: The robot is able to trace a figure eight on the grid, maintaining its path on a line at all times. 
                </p>
                <p></p>
                <p> <b> <big>Line Following:</big></b> </p>
                <p> Our robot was able to successfully navigate a line and correct to maintain a straight path.</p>
               <p><small><i>Basic Line Trace Video</i></small></p>
               <div style="position:relative;height:0;padding-bottom:75.0%"><iframe src="https://www.youtube.com/embed/mTXNq4K051o?ecver=2" width="480" height="360" frameborder="0" allow="autoplay; encrypted-media" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div> 
               <p></p>
                <p>The robot was able to make a strong correction to reach a line and then follow as well.</p>
               <p><small><i>Strong Correction Video</i></small></p>
               <div style="position:relative;height:0;padding-bottom:75.0%"><iframe src="https://www.youtube.com/embed/TKJXbjCIFHM?ecver=2" width="480" height="360" frameborder="0" allow="autoplay; encrypted-media" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>
               
               <p></p>
               <p> <b> <big>Figure Eight:</big></b> </p>
               <p> Our robot was able to successfully trace a figure eight on the grid.</p>
               <p><small><i>Figure Eight Video</i></small></p>
               <div style="position:relative;height:0;padding-bottom:75.0%"><iframe src="https://www.youtube.com/embed/KqPPQGsI_5g?ecver=2" width="480" height="360" frameborder="0" allow="autoplay; encrypted-media" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>
               
              </div>
              
              <div align = "left">
                <p> <b> <big>Further Explanation:</big></b> </p>
                <p> <b> Implementation:</b> </p>
                <p>Two line sensors are used to control the robot’s movement as it attempts to follow the line.  They are each attached to digital pins on the Arduino, with one initially starting on the white line and the other starting offset to the right on top of the darker material.  We have an initial averaging function that is called in the setup so we can establish threshold values for each sensor.  Once we obtain an average over 10 samples, we set threshold variables that we use for the remainder of the program.  Within the loop, we use conditionals to check when we need to correct to the left or right based on the running average that we continuously update.  With this implementation we successfully followed the white line, but struggled to consistently detect intersections where both sensors were on the white line.  For this reason, we decided to adjust the position of the sensors to make our algorithm more robust in the figure eight pattern. However, we had many issues using the digital sensors as it returned fluctuating and negative values that made it hard to establish a threshold. The solution of this problem was that the ISR was gotten rid of entirely because it didn’t work correctly (discussed more later). This implementation traced the line a little more reliably than the last, and turned at the intersections.</p>
                <p>In our final implementation we were able to establish stable thresholds and readings from the line sensors, allowing us to consistently correct and turn at intersections.  This was achieved with our improved digital pin readings (explained in section below).  Within our loop, we have simple conditional statements to check when either threshold is too low, or if both thresholds of the sensors are too low. After slightly adjusting delay times and speeds of the servo motors, we successfully made the robot drive in a figure eight pattern.</p>  
                <p> <b>Error and Solution to the ISR Sensor Implementation Problem:</b> </p>
                <p>Using a digital line sensor created many difficulties in the milestone. The first way the digital sensors were read was using an ISR that was triggered by new input, with each sensor connected to a digital pin. In the ISR, the sensor was read using the built-in micros() function, which measures microseconds since the arduino began running the current program with 4 microsecond precision. When both sensors were on the white line and too close, some type of error was encountered that resulted in negative numbers being read. The way in which the error was corrected was to use a different measurement system after debugging this issue for over an hour with the help of a TA with no improvements. </p>
                <p>The improved reading method is to use a function to set the “out” pin to an output, drive it high, then waits 10 microseconds and the pin is changed to an input. The function will return how many microseconds the pin was high for, between 0 and 3000 microseconds, with 4 microsecond precision. It was determined when the sensors were on the line or not by the use of a threshold value. If either sensor was above its threshold value then it would be determined that said sensor was off of the white line. The thresholds were tested using simple if statement conditionals which proved to be more accurate than while loops that were in a previous implementation.</p>
              </div>
             <div align = "middle">
               
               
               
               <p><b><big> MILESTONE 2:</big></b><p>
             <p> <b> <big>Objectives:</big></b> </p>
                <p>One: The robot is able to successfully circle an arbitrary set of walls. 
                </p>
                <p>Two: The robot is able to avoid other robots in its path. 
                </p>
               <p>Three: The robot can line track, avoid walls, and avoid other robots simultaneously. 
                </p>
                <p></p>
                <p> <b> <big>Wall Detection and Avoidance:</big></b> </p>
                <p> The robot is able to follow walls using thresholding on the solo sensor on the front. Once the robot reaches an intersection it reads the front, wall detecting IR sensor. If the read value is less than the threshold then there is a wall detected and the robot reacts. The react pattern is as follows: if no wall detected if front move forward (after any number of previous turns), if a wall detected turn left and recheck, if there is still a wall then turn around and recheck, and finally if there is still a wall after the 180 degree turn, turn right one last time and return the way it came.</p>
                <p> A video of operation can be seen below demonstrating proper line following as well as wall avoidance.</p>
               <div style="position:relative;height:0;padding-bottom:56.25%"><iframe src="https://www.youtube.com/embed/Wfx0UQ-Yn24?ecver=2" width="640" height="360" frameborder="0" allow="autoplay; encrypted-media" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>
               <p></p>
               <p> <b> <big>Avoiding Other Robots:</big></b> </p>
               <p> In order to avoid other robots, it was determined that the best algorithm for now would be to simply turn the robot 180 degrees and have it travel the opposite direction it was originally going upon detecting another robot.</p>
               <p>Rather than separately demonstrate the robot detection, this was integrated immediately to the overall system demonstrated in a video in the <b>Integrated System </b> section.</p>
               <p></p>
               <p> <b> <big>Integrated System:</big></b> </p>
               <p> The final integrated system was able to detect other robots and react to them, line trace, as well as avoid walls in the maze. In order to fully display what the action of the robot is two LEDs were implemented as well. The green LED is turned on for the duration that the robot is detecting and avoiding walls. The red LED is on for the duration that the robot is detecting another robot and reacting to the hazard.</p>
               <div style="position:relative;height:0;padding-bottom:56.25%"><iframe src="https://www.youtube.com/embed/2uqn-Q-XyeU?ecver=2" width="640" height="360" frameborder="0" allow="autoplay; encrypted-media" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>
               <p><small><i>Full Detection Video</i></small></p>
               
              <div align = "left">
                <p> <b> <big>Further Discussion:</big></b> </p>
                <p> <b> Implementation:</b> </p>
                <p> One primary note that is not immediately apparent in implementing this milestone was that the group decided the FFT for robot detection could be run at an interval. To achieve the interval run of the FFT code, a simple count condition was introduced in the loop, such that the variable counted up during the main function. Upon reaching a certain value (20 in this case) the FFT calculation would be triggered and the count reset.</p>
                <p> An additional word of caution is that once setting the ADMUX value, there is a finite amount of time that reading the value of the ADC will only give a garbage value. As such, the group's implementation simply read it once to clear the register, delayed for 20 milliseconds, and then read again to receive a valid value.</p>
               </div>
               
               
               
               
               <p><b><big> MILESTONE 3:</big></b><p>
             <p> <b> <big>Objectives:</big></b> </p>
                <p>One: Robot capable of maze exploration using DFS, BFS, Dijkstra, or A* 
                </p>
                <p>Two: The robot is also able to update the GUI 
                </p>
              
                <p></p>
                <p> <b> <big>Search Algorithm:</big></b> </p>
                <p>A DFS search algorithm was first started by adjusting the MATLAB example code for BFS to work with ArduinoIDE and then converting it to DFS. Variables for the starting position, array sizes, maze size, direction, and the arrays themselves were declared. </p>
               <div align = "middle"><img  src="img/algo_code.png" alt=""></div>
               <p><small><i>Search Algorithm Setup Code</i></small></p>
               <p>A  loop was created that would continue as long as the frontier array was not empty. A temporary variable popped the first element of the frontier out, and then the size of the frontier was decreased as to create a first in first out (FIFO) buffer. </p>
               <div align = "middle"><img  src="img/loop_code.png" alt=""></div>
               <p><small><i>Search Algorithm Execution Loop Code</i></small></p>
               <p>Afterwards a group of four if statements (one for each cardinal direction) would move the robot in the desired direction continuously as well as update the arrays appropriately until the robot could no longer move in said direction. At that point, the direction variable would update and the robot would move in a new direction until it could no longer do so. </p>
               <div align = "middle"><img  src="img/cond_code.png" alt=""></div>
               <p><small><i>Search Algorithm Conditionals Code</i></small></p>
               <p>In order to keep the robot from revisiting locations, a function called checkVisited would run each time the robot attempted to move that cycles through the visited array and checks if the proposed new location was already visited.</p>
               <div align = "middle"><img  src="img/vis_code.png" alt=""></div>
               <p><small><i>Visited Check Code</i></small></p>
               
               
               <p> <b> <big>Arduino Integration:</big></b> </p>
               <p>In order to integrate the search algorithm with the robot and allow for full functionality, the implementation of the Depth First Search was embedded in the logic when at an intersection. First, the robot adds its current position to the visited array and updates its own coordinates as seen here:</p>
                <div align = "middle"><img  src="img/visited_code.png" alt=""></div>
               <p><small><i>Add to Visited Array Code Snippit </i></small></p>
               <p>The robot then reads the wall sensors for all directions (Left, Right, and Straight), and depending on the presence of walls or not, the corresponding coordinates are retained in the neighbor array. To calculate the coordinates of each node to add to the neighbor array (given that there is no wall detected), the orientation is taken into account to determine if the robot is facing north, south, east, or west. Either the x or y coordinate is incremented or decremented from the current position based on this orientation and then added. The logic for this can be seen below (NOTE: Orientation is numbered counterclockwise starting at zero for East):</p>
              <div align = "middle"><img  src="img/neighbor_code.png" alt=""></div>
               <p><small><i>Add to Neighbor Array Code Snippit </i></small></p>
               <p>Once the neighbor array is updated, the robot then determines a new target index to reach by polling the neighbor array for the most recent entry. If the target index is within one unit of distance (Manhattan Distance used) of the current position, the robot orients to reach the appropriate address by comparing the x and y values of its position, the target position, and its current orientation. However, if the target is greater than one unit away, then the robot begins to backtrack through the visited array until the target is within one intersection. The function that handles this orienting can be seen below:</p>
               <div align = "middle"><img  src="img/orient_code.png" alt=""></div>
               <p><small><i>Align Function Code Snippit </i></small></p>
               <p>After orienting, the robot continues straight until it reaches the next intersection. The outlined process above is repeated continuously until all of the squares have been visited, at which point it has then explored all possible locations. A demonstration of operation can be seen in the videos below:</p>
               
               <div style="position:relative;height:0;padding-bottom:75.0%"><iframe src="https://www.youtube.com/embed/GRol_Tz_6bE?ecver=2" width="480" height="360" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>
               <p><small><i>Maze 1 Exploration </i></small></p>
               <div style="position:relative;height:0;padding-bottom:75.0%"><iframe src="https://www.youtube.com/embed/Vhf6ZWB_zI4?ecver=2" width="480" height="360" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>
               <p><small><i>Maze 2 Exploration </i></small></p>
               
               
               
               
               
               <p><b><big> MILESTONE 4:</big></b><p>
             <p> <b> <big>Objectives:</big></b> </p>
                <p>One: Robot which can detect when there are/are not treasures
                </p>
                <p>Two: Robot which can successfully distinguish between red and blue treasures
                </p>
               <p>Three: Robot which can successfully distinguish a square, triangle, and diamond shape
                </p>
              
                <p></p>
                <p> <b> <big>Detecting Treasures and Their Color:</big></b> </p>
                <p>Our method for detecting treasures uses the same method as described in Lab 4. </p>
               
               <p><small><i>Color and Treasure Detection</i></small></p>
               
               <p> <b> <big>Distinguishing Square, Triangle, and Diamond Shapes:</big></b> </p>
               <p>In order to detect shapes, it is helpful to implement good edge detection, which we do using grayscale filtering. We first attempt to do edge detection using an image convolution that will expose sharp edges in the camera output. To implement this, we compute each output pixel value by applying a convolution matrices to the input image pixels. The particular convolution kernel we attempt to mimic is the following:</p>
                
               <div align = "middle"><img  src="img/sobel.png" alt=""></div>
               <p><small><i>Sobel Filter </i></small></p>
               
               
               <p>To compute the output pixel value, we use a simpler version of the three by three matrix and instead just use the middle row to compute the output. For example, to compute the output values in the 3rd column, we multiply those values by 8 and elementwise subtract the values in the 2nd column and 4th column.  The edge values at the first and last column are computed using two columns instead of three.  In our implementation, we need to instantiate three different M9K blocks to compute the output values since each block contains only one read port. As we receive pixels from the camera, we write the values to each of the three memory blocks. Thus when we compute the output pixel values we can read from each of the three memory blocks and apply the convolution. </p>
               
               <p>It turns out that this method produces memory issues with the FPGA and would require other methods to compute the convolution in a smart way.  Since we would still need to perform processing on the edge exposed image, we instead decide to directly process the output image to achieve some basic functionality first.To distinguish between different shapes, we accumulate color counts at three horizontal lines across the image at different heights.  If the color counts are all approximately the same, we will know that the shape is a square.  If one color count is significantly different from the other, then we know that the shape is either a diamond or triangle.  Our implementation is able to reuse some of the code developed for color detection.  For three particular Y_ADDR, we accumulate counts for both the blue and red colors.  Then, we use another always block that is triggered by VSYNC to check the count values at the end of every image. In this block, we determine if the absolute value of the count differences between each of the three Y_ADDR is close enough to be a square, otherwise we classify it as a triangle or diamond. The following code demonstrates our method and implementation:</p>
                
               <div align = "middle"><img  src="img/orient_code.png" alt=""></div>
               <p><small><i>Edge Detection Code Snippit </i></small></p>
              
               
          </div>
        </div>
      </div>
    </section>


    <footer class="footer text-faded text-center py-5">
      <div class="container">
        <p class="m-0 small">Copyright &copy; Your Website 2018</p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  </body>

  <!-- Script to highlight the active date in the hours list -->
  <script>
    $('.list-hours li').eq(new Date().getDay()).addClass('today');
  </script>

</html>
